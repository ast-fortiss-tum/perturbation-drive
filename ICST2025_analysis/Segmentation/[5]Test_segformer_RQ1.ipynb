{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qCzfBeWYY4G2"
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def load_h5_to_dictionary(file_path):\n",
    "    data_dict = {}\n",
    "    with h5py.File(file_path, 'r') as hf:\n",
    "        for key in hf.keys():\n",
    "            print(key)\n",
    "            # Create a list to store the arrays for each key\n",
    "            array_list = []\n",
    "            # Get the group corresponding to the current key\n",
    "            group = hf[key]\n",
    "            for dataset_name in group.keys():\n",
    "                # Append each dataset (array) to the array list\n",
    "                array_list.append(np.array(group[dataset_name]))\n",
    "            # Store the array list for the current key in the dictionary\n",
    "            data_dict[key] = array_list\n",
    "    return data_dict\n",
    "\n",
    "def find_lowest_dimensions(images_list1, images_list2):\n",
    "    # Find the lowest height and width across both lists\n",
    "    lowest_height = min(images_list1[0].shape[0], images_list2[0].shape[0])\n",
    "    lowest_width = min(images_list1[0].shape[1], images_list2[0].shape[1])\n",
    "    return lowest_height, lowest_width\n",
    "\n",
    "def crop_images_to_lowest_dimensions(images_list, lowest_height, lowest_width):\n",
    "    cropped_images = [image[:lowest_height, :lowest_width,:] for image in images_list]\n",
    "    return cropped_images\n",
    "\n",
    "def crop_1d_to_lowest_dimensions(images_list, lowest_height, lowest_width):\n",
    "    cropped_images = [image[:lowest_height, :lowest_width] for image in images_list]\n",
    "    return cropped_images\n",
    "\n",
    "def map_to_r(number):\n",
    "    colorr = [255, 128, 0, 0, 128, 0, 0, 0, 128, 0, 0, 0, 128, 0, 128, 255, 128, 255, 0, 128]\n",
    "    if number < 20:\n",
    "        return int(colorr[number])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def map_to_g(number):\n",
    "    colorg = [0, 128, 255, 128, 0, 0, 128, 0, 0, 128, 0, 128, 0, 128, 255, 128, 0, 128, 255, 128]\n",
    "    if number < 20:\n",
    "        return int(colorg[number])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def map_to_b(number):\n",
    "    colorb = [0, 0, 0, 128, 128, 255, 255, 128, 0, 128, 128, 255, 128, 0, 0, 0, 128, 255, 255, 128]\n",
    "    if number < 20:\n",
    "        return int(colorb[number])\n",
    "    else:\n",
    "        return 255\n",
    "\n",
    "def map_to_desired_structure(input_image, output_image):\n",
    "    return {'pixel_values': input_image, 'labels': output_image}\n",
    "def map_to_desired_structure_weather(input_image, output_image, weather):\n",
    "    return {'pixel_values': input_image, 'labels': output_image, 'weather': weather}\n",
    "\n",
    "def map_values(value):\n",
    "    # if value in [1, 2, 6, 7, 8, 16, 21, 255]:\n",
    "    #     return 1\n",
    "    # elif value in [11, 12]:\n",
    "    #     return 2\n",
    "    # elif value in [13, 14, 15, 17]:\n",
    "    #     return 3\n",
    "    # elif value == 10:\n",
    "    #     return 4\n",
    "    # elif value == 0:\n",
    "    #     return 0\n",
    "    # else:\n",
    "    #     return 1\n",
    "    # if value==255:\n",
    "    #     value= 22\n",
    "    return value\n",
    "\n",
    "def generator(input_dict, output_dict, input_indices, input_dic_keys):\n",
    "    vectorized_map = np.vectorize(map_values)\n",
    "    for key in input_dic_keys:\n",
    "        for index in input_indices[key]:\n",
    "            if index < len(input_dict[key]):\n",
    "                input_image = input_dict[key][index]\n",
    "                output_image = output_dict[key][index]\n",
    "                \n",
    "                # Apply transformations lazily\n",
    "                input_image = tf.reverse(input_image, axis=[-1])\n",
    "                input_image = tf.transpose(input_image, (2, 0, 1))  # Adjust to (channels, height, width)\n",
    "                \n",
    "                # Vectorize and map values for the label\n",
    "                output_image = vectorized_map(output_image)\n",
    "\n",
    "                # Yield one image and label pair at a time\n",
    "                yield input_image, output_image\n",
    "\n",
    "def create_tf_dataset(input_dict, output_dict, input_indices, input_dic_keys, batch_size=5):\n",
    "    # Use a generator to load the data lazily\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: generator(input_dict, output_dict, input_indices, input_dic_keys),\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, None, None), dtype=tf.float32),  # Input image shape\n",
    "            tf.TensorSpec(shape=(None, None), dtype=tf.float32)          # Output image shape\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Preprocess dataset by mapping to the desired structure\n",
    "    dataset = dataset.map(map_to_desired_structure)\n",
    "\n",
    "    # Shuffle and batch the dataset\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)  # Load batches asynchronously\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def generator_pert(input_dict, output_dict, input_indices, input_dic_keys):\n",
    "    vectorized_map = np.vectorize(map_values)\n",
    "    # print(input_dict)\n",
    "    for key in input_dic_keys:\n",
    "        for key2 in input_indices[key]:\n",
    "            print(key2)\n",
    "            for index in input_indices[key][key2]:\n",
    "                if index < len(input_dict[key][key2]):\n",
    "                    input_image = input_dict[key][key2][index]\n",
    "                    output_image = output_dict[key][key2][index]\n",
    "                    \n",
    "                    # Apply transformations lazily\n",
    "                    input_image = tf.reverse(input_image, axis=[-1])\n",
    "                    input_image = tf.transpose(input_image, (2, 0, 1))  # Adjust to (channels, height, width)\n",
    "                    \n",
    "                    # Vectorize and map values for the label\n",
    "                    output_image = vectorized_map(output_image)\n",
    "\n",
    "                    # Yield one image and label pair at a time\n",
    "                    yield input_image, output_image, key2\n",
    "\n",
    "def create_tf_dataset_pert(input_dict, output_dict, input_indices, input_dic_keys, batch_size=5):\n",
    "    # Use a generator to load the data lazily\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: generator_pert(input_dict, output_dict, input_indices, input_dic_keys),\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, None, None), dtype=tf.float32),  # Input image shape\n",
    "            tf.TensorSpec(shape=(None, None), dtype=tf.float32),        # Output image shape\n",
    "            tf.TensorSpec(shape=(), dtype=tf.string)                    # String data shape\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Preprocess dataset by mapping to the desired structure\n",
    "    dataset = dataset.map(map_to_desired_structure_weather)\n",
    "\n",
    "    # Shuffle and batch the dataset\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)  # Load batches asynchronously\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def calculate_mse(gt_mask, pred_mask):\n",
    "    \"\"\"Calculates the Mean Squared Error between the ground truth mask and predicted mask.\"\"\"\n",
    "    return np.mean((gt_mask - pred_mask) ** 2)\n",
    "def calculate_iou(gt_mask, pred_mask, num_classes):\n",
    "    \"\"\"Calculates Intersection over Union (IoU) for each class.\"\"\"\n",
    "    iou_per_class = []\n",
    "    for class_id in range(num_classes):\n",
    "        # Create binary masks for the current class in both ground truth and prediction\n",
    "        gt_class_mask = (gt_mask == class_id)\n",
    "        pred_class_mask = (pred_mask == class_id)\n",
    "        \n",
    "        # Calculate intersection and union\n",
    "        intersection = np.logical_and(gt_class_mask, pred_class_mask).sum()\n",
    "        union = np.logical_or(gt_class_mask, pred_class_mask).sum()\n",
    "\n",
    "        # Avoid division by zero\n",
    "        if union == 0:\n",
    "            iou = 1.0  # If there are no ground truth or predicted pixels, IoU is perfect\n",
    "        else:\n",
    "            iou = intersection / union\n",
    "        \n",
    "        iou_per_class.append(iou)\n",
    "\n",
    "    return iou_per_class\n",
    "def calculate_f1(precision, recall):\n",
    "    return 2 * (precision * recall) / (precision + recall + 1e-7)\n",
    "def calculate_accuracy(gt, pred):\n",
    "    correct_predictions = tf.reduce_sum(tf.cast(tf.equal(gt, pred), dtype=tf.float32))\n",
    "    total_pixels = tf.cast(tf.size(gt), dtype=tf.float32)\n",
    "    return correct_predictions / total_pixels\n",
    "def calculate_recall(gt, pred):\n",
    "    # Cast tensors to float32\n",
    "    gt = tf.cast(gt, tf.float32)\n",
    "    pred = tf.cast(pred, tf.float32)\n",
    "    \n",
    "    true_positive = tf.reduce_sum(gt * pred)\n",
    "    false_negative = tf.reduce_sum(gt) - true_positive\n",
    "    return true_positive / (true_positive + false_negative + 1e-7)\n",
    "def calculate_precision(gt, pred):\n",
    "    # Cast tensors to float32\n",
    "    gt = tf.cast(gt, tf.float32)\n",
    "    pred = tf.cast(pred, tf.float32)\n",
    "    \n",
    "    true_positive = tf.reduce_sum(gt * pred)\n",
    "    false_positive = tf.reduce_sum(pred) - true_positive\n",
    "    return true_positive / (true_positive + false_positive + 1e-7)\n",
    "def calculate_dice(gt, pred):\n",
    "    # Cast the tensors to float32\n",
    "    gt = tf.cast(gt, tf.float32)\n",
    "    pred = tf.cast(pred, tf.float32)\n",
    "    \n",
    "    # Calculate the intersection and Dice coefficient\n",
    "    intersection = tf.reduce_sum(gt * pred)\n",
    "    return (2. * intersection) / (tf.reduce_sum(gt) + tf.reduce_sum(pred) + 1e-7)\n",
    "\n",
    "def calculate_f1_per_class(precision, recall, num_classes):\n",
    "    f1_per_class = []\n",
    "    for class_id in range(num_classes):\n",
    "        f1 = 2 * (precision[class_id] * recall[class_id]) / (precision[class_id] + recall[class_id] + 1e-7)\n",
    "        f1_per_class.append(f1)\n",
    "    return f1_per_class\n",
    "def calculate_accuracy_per_class(gt_mask, pred_mask, num_classes):\n",
    "    accuracy_per_class = []\n",
    "    for class_id in range(num_classes):\n",
    "        gt_class_mask = (gt_mask == class_id)\n",
    "        pred_class_mask = (pred_mask == class_id)\n",
    "        \n",
    "        correct_predictions = tf.reduce_sum(tf.cast(tf.equal(gt_class_mask, pred_class_mask), dtype=tf.float32))\n",
    "        total_pixels = tf.reduce_sum(tf.cast(gt_class_mask, tf.float32))\n",
    "        \n",
    "        if total_pixels == 0:\n",
    "            accuracy_per_class.append(1.0)  # If no pixels of this class, assume perfect accuracy\n",
    "        else:\n",
    "            accuracy_per_class.append(correct_predictions / total_pixels)\n",
    "    \n",
    "    return accuracy_per_class\n",
    "def calculate_recall_per_class(gt_mask, pred_mask, num_classes):\n",
    "    recall_per_class = []\n",
    "    for class_id in range(num_classes):\n",
    "        gt_class_mask = tf.cast(gt_mask == class_id, tf.float32)\n",
    "        pred_class_mask = tf.cast(pred_mask == class_id, tf.float32)\n",
    "        \n",
    "        true_positive = tf.reduce_sum(gt_class_mask * pred_class_mask)\n",
    "        false_negative = tf.reduce_sum(gt_class_mask) - true_positive\n",
    "        \n",
    "        if (true_positive + false_negative) == 0:\n",
    "            recall_per_class.append(1.0)\n",
    "        else:\n",
    "            recall_per_class.append(true_positive / (true_positive + false_negative + 1e-7))\n",
    "    \n",
    "    return recall_per_class\n",
    "def calculate_precision_per_class(gt_mask, pred_mask, num_classes):\n",
    "    precision_per_class = []\n",
    "    for class_id in range(num_classes):\n",
    "        gt_class_mask = tf.cast(gt_mask == class_id, tf.float32)\n",
    "        pred_class_mask = tf.cast(pred_mask == class_id, tf.float32)\n",
    "        \n",
    "        true_positive = tf.reduce_sum(gt_class_mask * pred_class_mask)\n",
    "        false_positive = tf.reduce_sum(pred_class_mask) - true_positive\n",
    "        \n",
    "        if (true_positive + false_positive) == 0:\n",
    "            precision_per_class.append(1.0)\n",
    "        else:\n",
    "            precision_per_class.append(true_positive / (true_positive + false_positive + 1e-7))\n",
    "    \n",
    "    return precision_per_class\n",
    "def calculate_dice_per_class(gt_mask, pred_mask, num_classes):\n",
    "    dice_per_class = []\n",
    "    for class_id in range(num_classes):\n",
    "        gt_class_mask = tf.cast(gt_mask == class_id, tf.float32)\n",
    "        pred_class_mask = tf.cast(pred_mask == class_id, tf.float32)\n",
    "        \n",
    "        intersection = tf.reduce_sum(gt_class_mask * pred_class_mask)\n",
    "        dice_score = (2. * intersection) / (tf.reduce_sum(gt_class_mask) + tf.reduce_sum(pred_class_mask) + 1e-7)\n",
    "        \n",
    "        dice_per_class.append(dice_score)\n",
    "    \n",
    "    return dice_per_class\n",
    "\n",
    "def display(display_list,label=\"output plots\"):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    print(label)\n",
    "    title = [\"Input Image\", \"True Mask\", \"Predicted Mask\",\"diff\"]\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i + 1)\n",
    "        plt.title(title[i])\n",
    "        # print(display_list[i].shape)\n",
    "        img=tf.keras.utils.array_to_img(display_list[i])\n",
    "        img_raw=display_list[i].numpy().astype(int)[:,:,0]\n",
    "        if i>0:\n",
    "            print(np.unique(img_raw))\n",
    "            colored_mask = np.zeros_like(tf.keras.utils.array_to_img(display_list[0]))\n",
    "            colored_mask[:, :, 0] = np.vectorize(map_to_b)(img_raw)\n",
    "            colored_mask[:, :, 1] = np.vectorize(map_to_g)(img_raw)\n",
    "            colored_mask[:, :, 2] = np.vectorize(map_to_r)(img_raw)\n",
    "            img=colored_mask\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def create_mask(pred_mask):\n",
    "    pred_mask = tf.math.argmax(pred_mask, axis=1)\n",
    "    pred_mask = tf.expand_dims(pred_mask, -1)\n",
    "    return pred_mask\n",
    "    \n",
    "def cast_to_int32(image):\n",
    "    # Normalize the image pixel values to [0, 1]\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    # Convert to uint8 by scaling the values back to [0, 255]\n",
    "    image = tf.cast(image * 255, tf.int32)\n",
    "    return image\n",
    "    \n",
    "\n",
    "def show_predictions(dataset=None, num=1):\n",
    "    if dataset:\n",
    "        for sample in dataset.take(num):\n",
    "            \n",
    "            images, masks = sample[\"pixel_values\"], sample[\"labels\"]\n",
    "            print(images.shape)\n",
    "            masks = tf.expand_dims(masks, -1)\n",
    "            pred_masks = model.predict(images).logits\n",
    "            images = tf.transpose(images, (0, 2, 3, 1))\n",
    "            created_masks=cast_to_int32(create_mask(pred_masks))\n",
    "            for i in range(0,len(created_masks)):\n",
    "                gt=cast_to_int32(masks[i])\n",
    "                created_mask=created_masks[i]\n",
    "                created_mask = tf.image.resize(created_mask, [374, 1238],\n",
    "                            method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "                \n",
    "\n",
    "                diff_mask_road=abs(created_mask-gt)\n",
    "                print(f\"MSE: {calculate_mse(gt, created_mask)}\")\n",
    "                print(f\"IOU: {calculate_iou(gt, created_mask,22)}\")\n",
    "                display([images[i], gt,created_mask,diff_mask_road ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_tp_fp_tn_fn(gt_flat, pred_flat, num_classes):\n",
    "    TP = np.zeros(num_classes)\n",
    "    FP = np.zeros(num_classes)\n",
    "    TN = np.zeros(num_classes)\n",
    "    FN = np.zeros(num_classes)\n",
    "\n",
    "    for class_idx in range(num_classes):\n",
    "        gt_class = (gt_flat == class_idx)\n",
    "        pred_class = (pred_flat == class_idx)\n",
    "        \n",
    "        TP[class_idx] += np.sum(np.logical_and(gt_class, pred_class))\n",
    "        FP[class_idx] += np.sum(np.logical_and(~gt_class, pred_class))\n",
    "        FN[class_idx] += np.sum(np.logical_and(gt_class, ~pred_class))\n",
    "        TN[class_idx] += np.sum(np.logical_and(~gt_class, ~pred_class))\n",
    "        \n",
    "    return TP, FP, TN, FN\n",
    "\n",
    "class DisplayCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, dataset, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        clear_output(wait=True)\n",
    "        show_predictions(self.dataset)\n",
    "        print(\"\\nSample Prediction after epoch {}\\n\".format(epoch + 1))\n",
    "\n",
    "def calculate_psnr(gt, pred, epsilon=1e-7):\n",
    "    mse = np.mean((gt - pred) ** 2)\n",
    "    if mse == 0:\n",
    "        return np.inf\n",
    "    max_pixel = np.max(gt)\n",
    "    psnr = 20 * np.log10(max_pixel / (np.sqrt(mse) + epsilon))\n",
    "    return psnr\n",
    "\n",
    "\n",
    "def calculate_mae(gt, pred):\n",
    "    return tf.reduce_mean(tf.abs(tf.cast(gt, tf.float32) - tf.cast(pred, tf.float32))).numpy()\n",
    "\n",
    "def calculate_ssim(gt, pred):\n",
    "    gt_img = gt.numpy().squeeze()\n",
    "    pred_img = pred.numpy().squeeze()\n",
    "    \n",
    "    # Ensure images are at least 1x1\n",
    "    if gt_img.size == 0 or pred_img.size == 0:\n",
    "        return 0  # or handle accordingly\n",
    "    \n",
    "    # Ensure images have at least two dimensions\n",
    "    if gt_img.ndim < 2 or pred_img.ndim < 2:\n",
    "        raise ValueError(\"Images must have at least two dimensions\")\n",
    "    \n",
    "    # Determine the minimum dimension size\n",
    "    min_dim = min(gt_img.shape[0], gt_img.shape[1])\n",
    "    \n",
    "    # Set win_size appropriately\n",
    "    win_size = min(7, min_dim)\n",
    "    if win_size % 2 == 0:\n",
    "        win_size = max(win_size - 1, 1)  # Ensure win_size is at least 1\n",
    "    \n",
    "    # Handle multichannel images\n",
    "    if gt_img.ndim == 3:\n",
    "        channel_axis = -1  # Assuming channels are in the last axis\n",
    "    else:\n",
    "        channel_axis = None  # Grayscale images\n",
    "    \n",
    "    # Compute SSIM\n",
    "    ssim_value = ssim(\n",
    "        gt_img,\n",
    "        pred_img,\n",
    "        data_range=gt_img.max() - gt_img.min(),\n",
    "        win_size=win_size,\n",
    "        channel_axis=channel_axis\n",
    "    )\n",
    "    return ssim_value\n",
    "def show_metrics_extended_pert(dataset=None, num_classes=22, model=None, output_dir='metrics_logs', diff_images_dir='segmentation_differences', has_weather=True,pert=\"na\"):\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "\n",
    "    epsilon = 1e-7  # Assign at the beginning\n",
    "\n",
    "    if dataset:\n",
    "        total_samples = 0\n",
    "\n",
    "        # Initialize accumulators per weather type\n",
    "        weather_metrics = {}\n",
    "\n",
    "        # Create directories if they don't exist\n",
    "        # if not os.path.exists(diff_images_dir):\n",
    "        #     os.makedirs(diff_images_dir)\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        sample_index = 0  # To name the saved images uniquely\n",
    "\n",
    "        for batch in dataset:\n",
    "            if has_weather:\n",
    "                images, masks, weather = batch[\"pixel_values\"], batch[\"labels\"], batch[\"weather\"]\n",
    "                unique_weathers = np.unique(weather)\n",
    "\n",
    "                for w in unique_weathers:\n",
    "                    # Filter images and masks by the current weather type\n",
    "                    mask_for_weather = weather == w\n",
    "                    images_weather = images[mask_for_weather]\n",
    "                    masks_weather = masks[mask_for_weather]\n",
    "\n",
    "                    if w not in weather_metrics:\n",
    "                        weather_metrics[w] = {\n",
    "                            'TP_total': np.zeros(num_classes),\n",
    "                            'FP_total': np.zeros(num_classes),\n",
    "                            'TN_total': np.zeros(num_classes),\n",
    "                            'FN_total': np.zeros(num_classes),\n",
    "                            'total_mse': 0,\n",
    "                            'total_mae': 0,\n",
    "                            'total_pixel_accuracy': 0,\n",
    "                            'total_pixels_considered': 0,\n",
    "                            'per_class_metrics': {class_idx: {'TP': 0, 'FP': 0, 'FN': 0, 'TN': 0} for class_idx in range(num_classes)}\n",
    "                        }\n",
    "\n",
    "                    # Process the batch for this specific weather\n",
    "                    masks_weather = tf.expand_dims(masks_weather, -1)\n",
    "                    pred_logits = model.predict(images_weather).logits\n",
    "                    images_weather = tf.transpose(images_weather, (0, 2, 3, 1))\n",
    "                    pred_masks = create_mask(pred_logits)\n",
    "\n",
    "                    gt = cast_to_int32(masks_weather)\n",
    "                    pred = cast_to_int32(pred_masks)\n",
    "\n",
    "                    pred = tf.image.resize(pred, [374, 1238], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "                    gt = tf.image.resize(gt, [374, 1238], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "\n",
    "                    gt_flat = tf.reshape(gt, [-1]).numpy()\n",
    "                    pred_flat = tf.reshape(pred, [-1]).numpy()\n",
    "\n",
    "                    valid_mask = gt_flat != 255\n",
    "                    gt_filtered = gt_flat[valid_mask]\n",
    "                    pred_filtered = pred_flat[valid_mask]\n",
    "\n",
    "                    mse_value = np.mean((gt_filtered - pred_filtered) ** 2)\n",
    "                    mae_value = np.mean(np.abs(gt_filtered - pred_filtered))\n",
    "\n",
    "                    weather_metrics[w]['total_mse'] += mse_value * len(gt_filtered)\n",
    "                    weather_metrics[w]['total_mae'] += mae_value * len(gt_filtered)\n",
    "                    weather_metrics[w]['total_pixels_considered'] += len(gt_filtered)\n",
    "\n",
    "                    TP, FP, TN, FN = calculate_tp_fp_tn_fn(gt_filtered, pred_filtered, num_classes)\n",
    "                    weather_metrics[w]['TP_total'] += TP\n",
    "                    weather_metrics[w]['FP_total'] += FP\n",
    "                    weather_metrics[w]['TN_total'] += TN\n",
    "                    weather_metrics[w]['FN_total'] += FN\n",
    "\n",
    "                    correct_pixels = np.sum(gt_filtered == pred_filtered)\n",
    "                    pixel_accuracy = correct_pixels / (len(gt_filtered) + epsilon)\n",
    "                    weather_metrics[w]['total_pixel_accuracy'] += pixel_accuracy * len(gt_filtered)\n",
    "\n",
    "                    # Accumulate per-class metrics\n",
    "                    for class_idx in range(num_classes):\n",
    "                        weather_metrics[w]['per_class_metrics'][class_idx]['TP'] += TP[class_idx]\n",
    "                        weather_metrics[w]['per_class_metrics'][class_idx]['FP'] += FP[class_idx]\n",
    "                        weather_metrics[w]['per_class_metrics'][class_idx]['FN'] += FN[class_idx]\n",
    "                        weather_metrics[w]['per_class_metrics'][class_idx]['TN'] += TN[class_idx]\n",
    "\n",
    "                    # Save difference images (for valid pixels)\n",
    "                    difference = np.abs(gt_filtered - pred_filtered)\n",
    "                    difference_image = np.zeros_like(gt_flat)\n",
    "                    difference_image[valid_mask] = difference\n",
    "                    difference_image = difference_image.reshape(gt.shape[0], gt.shape[1], gt.shape[2])\n",
    "                    difference_image = difference_image.astype(np.uint8) * (255 // num_classes)\n",
    "                    char = \"'\"\n",
    "                    weather_string = str(w).split(char)[1]\n",
    "                    # if not os.path.exists(os.path.join(diff_images_dir, weather_string)):\n",
    "                    #     os.makedirs(os.path.join(diff_images_dir, weather_string))\n",
    "                    for i in range(difference_image.shape[0]):\n",
    "                        # image_path = os.path.join(diff_images_dir, f'{weather_string}/{sample_index}.png')\n",
    "                        # Image.fromarray(difference_image[i].squeeze(), mode='L').save(image_path)\n",
    "                        sample_index += 1\n",
    "\n",
    "                    total_samples += gt.shape[0]\n",
    "\n",
    "                    if total_samples % 100 == 0:\n",
    "                        print(f\"Processed {total_samples} samples\")\n",
    "\n",
    "        # After processing the batches, calculate metrics for each weather type\n",
    "        overall_weather_metrics = {}\n",
    "        per_class_weather_metrics = {}\n",
    "\n",
    "        for w, metrics in weather_metrics.items():\n",
    "            TP_total = metrics['TP_total']\n",
    "            FP_total = metrics['FP_total']\n",
    "            TN_total = metrics['TN_total']\n",
    "            FN_total = metrics['FN_total']\n",
    "\n",
    "            precision = TP_total / (TP_total + FP_total + epsilon)\n",
    "            recall = TP_total / (TP_total + FN_total + epsilon)\n",
    "            f1_score = 2 * precision * recall / (precision + recall + epsilon)\n",
    "            accuracy = (TP_total + TN_total) / (TP_total + FP_total + TN_total + FN_total + epsilon)\n",
    "            iou = TP_total / (TP_total + FP_total + FN_total + epsilon)\n",
    "            dice = 2 * TP_total / (2 * TP_total + FP_total + FN_total + epsilon)\n",
    "            specificity = TN_total / (TN_total + FP_total + epsilon)\n",
    "\n",
    "            avg_mse = metrics['total_mse'] / (metrics['total_pixels_considered'] + epsilon)\n",
    "            avg_mae = metrics['total_mae'] / (metrics['total_pixels_considered'] + epsilon)\n",
    "            avg_pixel_accuracy = metrics['total_pixel_accuracy'] / (metrics['total_pixels_considered'] + epsilon)\n",
    "\n",
    "            relevant_TP = TP_total.sum()\n",
    "            relevant_FP = FP_total.sum()\n",
    "            relevant_TN = TN_total.sum()\n",
    "            relevant_FN = FN_total.sum()\n",
    "\n",
    "            overall_precision = relevant_TP / (relevant_TP + relevant_FP + epsilon)\n",
    "            overall_recall = relevant_TP / (relevant_TP + relevant_FN + epsilon)\n",
    "            overall_specificity = relevant_TN / (relevant_TN + relevant_FP + epsilon)\n",
    "            overall_accuracy = (relevant_TP + relevant_TN) / (relevant_TP + relevant_FP + relevant_FN + relevant_TN + epsilon)\n",
    "            overall_f1_score = 2 * overall_precision * overall_recall / (overall_precision + overall_recall + epsilon)\n",
    "            overall_iou = relevant_TP / (relevant_TP + relevant_FP + relevant_FN + epsilon)\n",
    "            overall_dice = 2 * relevant_TP / (2 * relevant_TP + relevant_FP + relevant_FN + epsilon)\n",
    "\n",
    "            # Store overall metrics for weather\n",
    "            overall_weather_metrics[w] = {\n",
    "                'Weather Type': w,\n",
    "                'Average MSE': avg_mse,\n",
    "                'Average MAE': avg_mae,\n",
    "                'Average Pixel Accuracy': avg_pixel_accuracy,\n",
    "                'Overall Precision': overall_precision,\n",
    "                'Overall Recall': overall_recall,\n",
    "                'Overall Specificity': overall_specificity,\n",
    "                'Overall F1 Score': overall_f1_score,\n",
    "                'Overall Accuracy': overall_accuracy,\n",
    "                'Overall IoU': overall_iou,\n",
    "                'Overall Dice Coefficient': overall_dice,\n",
    "            }\n",
    "\n",
    "            # Calculate per-class metrics for each weather type\n",
    "            per_class_metrics = []\n",
    "            for class_idx in range(num_classes):\n",
    "                per_class_TP = metrics['per_class_metrics'][class_idx]['TP']\n",
    "                per_class_FP = metrics['per_class_metrics'][class_idx]['FP']\n",
    "                per_class_FN = metrics['per_class_metrics'][class_idx]['FN']\n",
    "                per_class_TN = metrics['per_class_metrics'][class_idx]['TN']\n",
    "\n",
    "                precision_class = per_class_TP / (per_class_TP + per_class_FP + epsilon)\n",
    "                recall_class = per_class_TP / (per_class_TP + per_class_FN + epsilon)\n",
    "                f1_class = 2 * precision_class * recall_class / (precision_class + recall_class + epsilon)\n",
    "                iou_class = per_class_TP / (per_class_TP + per_class_FP + per_class_FN + epsilon)\n",
    "                dice_class = 2 * per_class_TP / (2 * per_class_TP + per_class_FP + per_class_FN + epsilon)\n",
    "\n",
    "                per_class_metrics.append({\n",
    "                    'Class': class_idx,\n",
    "                    'Precision': precision_class,\n",
    "                    'Recall': recall_class,\n",
    "                    'F1 Score': f1_class,\n",
    "                    'IoU': iou_class,\n",
    "                    'Dice Coefficient': dice_class\n",
    "                })\n",
    "\n",
    "            # Store per-class metrics for this weather type\n",
    "            per_class_weather_metrics[w] = pd.DataFrame(per_class_metrics)\n",
    "\n",
    "        # Convert to DataFrame and save to CSV\n",
    "        overall_weather_metrics_df = pd.DataFrame(overall_weather_metrics).T\n",
    "        overall_weather_metrics_df.to_csv(os.path.join(output_dir, f'overall_pert_metrics_({pert}).csv'), index=False)\n",
    "\n",
    "        for w, per_class_df in per_class_weather_metrics.items():\n",
    "            per_class_df.to_csv(os.path.join(output_dir, f'per_class_metrics_({w}).csv'), index=False)\n",
    "\n",
    "        print(\"\\nOverall Metrics by Weather Type:\")\n",
    "        print(overall_weather_metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  TFSegformerForSemanticSegmentation\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "model_checkpoint = \"nvidia/segformer-b0-finetuned-cityscapes-640-1280\"\n",
    "\n",
    "# id2label mapping for the KITTI dataset\n",
    "label2id = {\n",
    "        'Road': 0,\n",
    "        'Sidewalk': 1,\n",
    "        'Building': 2,\n",
    "        'Wall': 3,\n",
    "        'Fence': 4,\n",
    "        'Pole': 5,\n",
    "        'TrafficLight': 6,\n",
    "        'TrafficSign': 7,\n",
    "        'Vegetation': 8,\n",
    "        'Terrain': 9,\n",
    "        'Sky': 10,\n",
    "        'Person': 11,\n",
    "        'Rider': 12,\n",
    "        'Car': 13,\n",
    "        'Truck': 14,\n",
    "        'Bus': 15,\n",
    "        'Train': 16,\n",
    "        'Motorcycle': 17,\n",
    "        'Bicycle': 18,\n",
    "        'Void': 255,\n",
    "        'Misc': 19,\n",
    "        'GuardRail': 20,\n",
    "    }\n",
    "id2label = {label: id for id, label in label2id.items()}\n",
    "num_labels = len(id2label)\n",
    "print(len(id2label))\n",
    "model = TFSegformerForSemanticSegmentation.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "lr = 0.00006\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "model.compile(optimizer=optimizer)\n",
    "checkpoint_file_path = \"./content/Model_original.hdf5\"\n",
    "model.load_weights(checkpoint_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbations=['canny_edges_mapping','contrast','cutout_filter','defocus_blur','dotted_lines_mapping','elastic','false_color_filter','fog_filter','frost_filter','gaussian_blur','gaussian_noise','glass_blur','grayscale_filter','histogram_equalisation','impulse_noise','increase_brightness','jpeg_filter','low_pass_filter','motion_blur','phase_scrambling','poisson_noise','reflection_filter','sample_pairing_filter','saturation_decrease_filter','saturation_filter','scale_image','sharpen_filter','snow_filter','speckle_noise_filter','splatter_mapping','translate_image','white_balance_filter','zigzag_mapping']\n",
    "for perturbation in perturbations:\n",
    "    loaded_dictionary_images_sim={}\n",
    "    loaded_semantic_id_sim={}\n",
    "    file_path = f'/Volumes/Volume/new_perts/({perturbation})_raw_image_sim.h5'\n",
    "    loaded_dictionary_images_sim[perturbation]=load_h5_to_dictionary(file_path)\n",
    "    file_path = f'/Volumes/Volume/new_perts/({perturbation})_semantic_id_list_sim.h5'\n",
    "    loaded_semantic_id_sim[perturbation]=load_h5_to_dictionary(file_path)\n",
    "    internal_test_index_list=list(loaded_dictionary_images_sim[perturbation])\n",
    "    print(internal_test_index_list)\n",
    "    lowest_height=374\n",
    "    lowest_width=1238 \n",
    "    for dataset_index in internal_test_index_list:\n",
    "        loaded_dictionary_images_sim[perturbation][dataset_index] = crop_images_to_lowest_dimensions(loaded_dictionary_images_sim[perturbation][dataset_index], lowest_height, lowest_width)\n",
    "        loaded_semantic_id_sim[perturbation][dataset_index] = crop_1d_to_lowest_dimensions(loaded_semantic_id_sim[perturbation][dataset_index], lowest_height, lowest_width)\n",
    "\n",
    "    test_indexes_sim = {}\n",
    "    test_indexes_weather = {}\n",
    "    indexes_real = {}\n",
    "    train_indexes_sim_perturbed = {}\n",
    "    valid_indexes_sim_perturbed = {}\n",
    "    test_dataset_inner = {}\n",
    "    for dataset_index in internal_test_index_list:\n",
    "      test_dataset_inner[dataset_index]=[]\n",
    "      for index in range(1, len(loaded_dictionary_images_sim[perturbation][dataset_index])):\n",
    "        test_dataset_inner[dataset_index].append(index)\n",
    "    test_indexes_sim[perturbation]=test_dataset_inner\n",
    "    print(\"here\")\n",
    "    print(internal_test_index_list)\n",
    "    for dataset_index in internal_test_index_list:\n",
    "        print(\"Dataset\", perturbation,dataset_index)\n",
    "        print(\"Test sim: \",len(test_indexes_sim[perturbation][dataset_index]))\n",
    "\n",
    "    tf_dataset_test_sim = create_tf_dataset_pert(loaded_dictionary_images_sim, loaded_semantic_id_sim,test_indexes_sim,[perturbation])\n",
    "    test_dataset =tf_dataset_test_sim\n",
    "    auto = tf.data.AUTOTUNE\n",
    "\n",
    "\n",
    "    test_ds = (\n",
    "        test_dataset\n",
    "        .prefetch(auto)\n",
    "    )\n",
    "    show_metrics_extended_pert(test_ds,num_classes=22, model=model, output_dir='./content/evaluation/model_pert_perclass', diff_images_dir='./content/evaluation/model_pert_segmentations_perclass',has_weather=True,pert=perturbation)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tfEnv4",
   "language": "python",
   "name": "tfenv4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0f122d49862b437b95c0317503acee4b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_510deb9127754ec8b1a06ca73f5acab2",
      "max": 14563152,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_632e31d0691941b0a649e8de5baec3fc",
      "value": 14563152
     }
    },
    "1d843e18dab742b4b58bf50ff426fdd2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2157955bf53646e7b347adec3e44aa4a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a02a5f4a625405886793b93856f12d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3f60f87dab934d89b198e65e023bc9b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "510deb9127754ec8b1a06ca73f5acab2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "632e31d0691941b0a649e8de5baec3fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "70e51f43aca14059b8e6ff8cd8828bbf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7255ecf9a6e44ae6a1d2d317c7d18169": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75367f7d21824749baf627dbc77ca7ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "792bfcb81195415a820191e0b07c923f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7ee71a68ae704d7491f9e5371e5cd526": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c93d7f579d9342f7a3a69bf75fb0ac36",
      "placeholder": "",
      "style": "IPY_MODEL_3f60f87dab934d89b198e65e023bc9b3",
      "value": " 14.6M/14.6M [00:00&lt;00:00, 44.7MB/s]"
     }
    },
    "9e812626ca3a457fbbb92cf0df6c7f57": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a21248437cb142aab84404b2e64203fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a71b7a587bd64dbfa8c8fb4973625142": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_70e51f43aca14059b8e6ff8cd8828bbf",
      "max": 70043,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2a02a5f4a625405886793b93856f12d1",
      "value": 70043
     }
    },
    "a7cfeef29aa443a383f839024bd9377a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_af859a465f1d40d186c0dfa9facdadf2",
       "IPY_MODEL_0f122d49862b437b95c0317503acee4b",
       "IPY_MODEL_7ee71a68ae704d7491f9e5371e5cd526"
      ],
      "layout": "IPY_MODEL_7255ecf9a6e44ae6a1d2d317c7d18169"
     }
    },
    "af859a465f1d40d186c0dfa9facdadf2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2157955bf53646e7b347adec3e44aa4a",
      "placeholder": "",
      "style": "IPY_MODEL_75367f7d21824749baf627dbc77ca7ae",
      "value": "Downloading tf_model.h5: 100%"
     }
    },
    "b70c274750a04976b738a463a3962965": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_792bfcb81195415a820191e0b07c923f",
      "placeholder": "",
      "style": "IPY_MODEL_c9f338ae5d1940b5a77b612430bf9984",
      "value": " 70.0k/70.0k [00:00&lt;00:00, 1.46MB/s]"
     }
    },
    "c1a23b93eed54aadabcf3284553272f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1d843e18dab742b4b58bf50ff426fdd2",
      "placeholder": "",
      "style": "IPY_MODEL_a21248437cb142aab84404b2e64203fc",
      "value": "Downloading ()lve/main/config.json: 100%"
     }
    },
    "c93d7f579d9342f7a3a69bf75fb0ac36": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9f338ae5d1940b5a77b612430bf9984": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dd38f19dd15a40cf9c6e9129b4788578": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c1a23b93eed54aadabcf3284553272f9",
       "IPY_MODEL_a71b7a587bd64dbfa8c8fb4973625142",
       "IPY_MODEL_b70c274750a04976b738a463a3962965"
      ],
      "layout": "IPY_MODEL_9e812626ca3a457fbbb92cf0df6c7f57"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
